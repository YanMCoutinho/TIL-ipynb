{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09c42e9",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0da37",
   "metadata": {},
   "source": [
    "## Instalação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a50d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (3.10.3)\n",
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: seaborn in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (0.13.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (1.15.3)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (0.14.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from gdown)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting requests[socks] (from gdown)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm (from gdown)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4->gdown)\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests[socks]->gdown)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests[socks]->gdown)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests[socks]->gdown)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests[socks]->gdown)\n",
      "  Downloading certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\inteli\\downloads\\prova\\prova\\.venv\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.7.0-cp313-cp313-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/10.7 MB 4.9 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/10.7 MB 4.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/10.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/10.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.7/10.7 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.5/10.7 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.2/10.7 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.8/10.7 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.8/10.7 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.6/10.7 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.7/10.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.2/10.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 3.7 MB/s eta 0:00:00\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl (105 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, threadpoolctl, soupsieve, PySocks, networkx, joblib, idna, filelock, charset_normalizer, certifi, scikit-learn, requests, beautifulsoup4, gdown\n",
      "\n",
      "   ----------------------------------------  0/16 [urllib3]\n",
      "   ----- ----------------------------------  2/16 [tqdm]\n",
      "   ----- ----------------------------------  2/16 [tqdm]\n",
      "   ------------ ---------------------------  5/16 [PySocks]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   --------------- ------------------------  6/16 [networkx]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   -------------------- -------------------  8/16 [idna]\n",
      "   -------------------- -------------------  8/16 [idna]\n",
      "   ------------------------- -------------- 10/16 [charset_normalizer]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   ------------------------------ --------- 12/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [requests]\n",
      "   -------------------------------- ------- 13/16 [requests]\n",
      "   ----------------------------------- ---- 14/16 [beautifulsoup4]\n",
      "   ----------------------------------- ---- 14/16 [beautifulsoup4]\n",
      "   ----------------------------------- ---- 14/16 [beautifulsoup4]\n",
      "   ----------------------------------- ---- 14/16 [beautifulsoup4]\n",
      "   ----------------------------------- ---- 14/16 [beautifulsoup4]\n",
      "   ---------------------------------------- 16/16 [gdown]\n",
      "\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 certifi-2025.6.15 charset_normalizer-3.4.2 filelock-3.18.0 gdown-5.2.0 idna-3.10 joblib-1.5.1 networkx-3.5 requests-2.32.4 scikit-learn-1.7.0 soupsieve-2.7 threadpoolctl-3.6.0 tqdm-4.67.1 typing-extensions-4.14.0 urllib3-2.4.0\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy matplotlib gdown networkx seaborn scikit-learn scipy statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217314d",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from ChartGenerator import ChartGenerator\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency, ttest_ind, zscore as _zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be647a74",
   "metadata": {},
   "source": [
    "# Download do Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_arquivo = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b79b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to retrieve file url:\n",
      "\n",
      "\tCannot retrieve the public link of the file. You may need to change\n",
      "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
      "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\thttps://drive.google.com/uc?id=[link]\n",
      "\n",
      "but Gdown can't. Please check connections and permissions.\n"
     ]
    }
   ],
   "source": [
    "!gdown [link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4813a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnome_arquivo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Coutinho Yan\\Downloads\\prova\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Coutinho Yan\\Downloads\\prova\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Coutinho Yan\\Downloads\\prova\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Coutinho Yan\\Downloads\\prova\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Coutinho Yan\\Downloads\\prova\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(nome_arquivo)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecbf339",
   "metadata": {},
   "source": [
    "# Instruções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ba148",
   "metadata": {},
   "source": [
    "# Análise Estatistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estatistica_descritiva_numerica(df):\n",
    "    \"\"\"\n",
    "    Gera estatísticas descritivas e plot de histograma e boxplot\n",
    "    para todas as colunas numéricas do DataFrame.\n",
    "    \"\"\"\n",
    "    num_cols = df.select_dtypes(include=np.number).columns\n",
    "    stats = df[num_cols].describe().T\n",
    "    display(stats)  # exibe tabela de estatísticas\n",
    "    \n",
    "    for col in num_cols:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        sns.histplot(df[col], ax=axes[0], kde=True, color='skyblue')\n",
    "        axes[0].set_title(f'Histograma de {col}')\n",
    "        \n",
    "        sns.boxplot(x=df[col], ax=axes[1], color='lightgreen')\n",
    "        axes[1].set_title(f'Boxplot de {col}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def estatistica_descritiva_categorica(df):\n",
    "    \"\"\"\n",
    "    Gera estatísticas descritivas (contagem, %)\n",
    "    e plot de barras para todas as colunas categóricas do DataFrame.\n",
    "    \"\"\"\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in cat_cols:\n",
    "        counts = df[col].value_counts()\n",
    "        pct = df[col].value_counts(normalize=True).mul(100).round(2)\n",
    "        summary = pd.DataFrame({'count': counts, 'percent (%)': pct})\n",
    "        display(summary)\n",
    "        \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.countplot(data=df, x=col, order=counts.index, palette='pastel')\n",
    "        plt.title(f'Distribuição de {col}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2986a24b",
   "metadata": {},
   "source": [
    "# Matriz de Markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e47eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_matriz_de_markov(df, coluna_estado):\n",
    "    \"\"\"\n",
    "    Constrói a matriz de transição de Markov a partir de uma coluna de estados\n",
    "    em ordem temporal (linhas consecutivas).\n",
    "    Retorna um DataFrame onde cada linha é o estado anterior e cada coluna o estado seguinte,\n",
    "    preenchido com probabilidades de transição.\n",
    "    \"\"\"\n",
    "    estados = df[coluna_estado]\n",
    "    anterior = estados.shift(1)\n",
    "    atual = estados\n",
    "    contagens = pd.crosstab(anterior, atual) \\\n",
    "                  .drop(index=[np.nan], errors='ignore') \\\n",
    "                  .fillna(0)\n",
    "    matriz = contagens.div(contagens.sum(axis=1), axis=0)\n",
    "    return matriz\n",
    "\n",
    "def plotar_grafo_markov(matriz, limiar=0.0, layout='spring', figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Recebe uma matriz de Markov (DataFrame de probabilidades) e plota\n",
    "    um grafo direcionado com pesos proporcionais às probabilidades.\n",
    "     - limiar: probabilidade mínima para exibir a aresta\n",
    "     - layout: 'spring', 'circular' ou 'shell'\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    for origem in matriz.index:\n",
    "        for destino in matriz.columns:\n",
    "            p = matriz.at[origem, destino]\n",
    "            if p > limiar:\n",
    "                G.add_edge(origem, destino, weight=p)\n",
    "\n",
    "    if G.number_of_edges() == 0:\n",
    "        print(\"Não há transições acima do limiar especificado.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if layout == 'circular':\n",
    "        pos = nx.circular_layout(G)\n",
    "    elif layout == 'shell':\n",
    "        pos = nx.shell_layout(G)\n",
    "    else:\n",
    "        pos = nx.spring_layout(G)\n",
    "\n",
    "    pesos = nx.get_edge_attributes(G, 'weight')\n",
    "    # desenha nós e rótulos\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=1500, node_color='lightblue')\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "    # desenha arestas com largura proporcional à probabilidade\n",
    "    larguras = [v * 5 for v in pesos.values()]\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos,\n",
    "        arrowstyle='->',\n",
    "        arrowsize=12,\n",
    "        width=larguras,\n",
    "        edge_color='gray'\n",
    "    )\n",
    "    # escreve os valores de probabilidade nas arestas\n",
    "    etiquetas = {k: f\"{v:.2f}\" for k, v in pesos.items()}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=etiquetas, font_size=8)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da43923",
   "metadata": {},
   "source": [
    "# Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(func, distributions, n_sim=1000, summary=True, plot=True, bins=30):\n",
    "    \"\"\"\n",
    "    Gera simulação de Monte Carlo genérica.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - func: função a ser simulada; deve aceitar argumentos nomeados.\n",
    "    - distributions: dict onde cada chave é nome de argumento de func e o valor é\n",
    "      um callable que retorna uma amostra aleatória para esse argumento.\n",
    "    - n_sim: número de iterações da simulação.\n",
    "    - summary: se True, exibe estatísticas descritivas dos resultados.\n",
    "    - plot: se True, plota histograma dos resultados (válido para saída única).\n",
    "    - bins: número de bins do histograma.\n",
    "    \n",
    "    Retorno:\n",
    "    DataFrame com os resultados da simulação.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for _ in range(n_sim):\n",
    "        # amostra parâmetros\n",
    "        params = {name: samp() for name, samp in distributions.items()}\n",
    "        out = func(**params)\n",
    "        # organiza saída\n",
    "        if isinstance(out, dict):\n",
    "            results.append(out)\n",
    "        elif isinstance(out, (tuple, list)):\n",
    "            results.append({f'output_{i}': v for i, v in enumerate(out)})\n",
    "        else:\n",
    "            results.append({'result': out})\n",
    "    df_sim = pd.DataFrame(results)\n",
    "    \n",
    "    if summary:\n",
    "        print(df_sim.describe().T)\n",
    "    if plot and df_sim.shape[1] == 1:\n",
    "        col = df_sim.columns[0]\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(df_sim[col], bins=bins, kde=True, color='skyblue')\n",
    "        plt.title(f'Distribuição de {col}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return df_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec740e",
   "metadata": {},
   "source": [
    "# Avaliação dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f998a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teste_chi_quadrado(df, col1, col2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Teste de independência qui-quadrado entre duas variáveis categóricas.\n",
    "    Retorna estatística, p-valor, graus de liberdade, frequências esperadas e decisão.\n",
    "    \"\"\"\n",
    "    tabela = pd.crosstab(df[col1], df[col2])\n",
    "    chi2, p, dof, expected = chi2_contingency(tabela)\n",
    "    decisao = \"rejeita H0 (não-independentes)\" if p < alpha else \"falha em rejeitar H0 (independentes)\"\n",
    "    return {\n",
    "        \"chi2\": chi2,\n",
    "        \"p_value\": p,\n",
    "        \"dof\": dof,\n",
    "        \"expected_freq\": expected,\n",
    "        \"decisão\": decisao\n",
    "    }\n",
    "\n",
    "def calcular_z_score(serie):\n",
    "    \"\"\"\n",
    "    Calcula o z-score de uma série numérica.\n",
    "    Retorna uma série com o mesmo índice.\n",
    "    \"\"\"\n",
    "    return pd.Series(_zscore(serie, nan_policy='omit'), index=serie.index)\n",
    "\n",
    "def teste_t_student(df, coluna_valor, coluna_grupo, grupo1, grupo2, equal_var=True, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Teste t-Student para duas amostras independentes.\n",
    "    - coluna_valor: coluna numérica\n",
    "    - coluna_grupo: coluna categórica com os dois grupos\n",
    "    - grupo1, grupo2: nomes dos grupos a comparar\n",
    "    Retorna estatística t, p-valor e decisão.\n",
    "    \"\"\"\n",
    "    amostra1 = df[df[coluna_grupo] == grupo1][coluna_valor]\n",
    "    amostra2 = df[df[coluna_grupo] == grupo2][coluna_valor]\n",
    "    t_stat, p = ttest_ind(amostra1, amostra2, equal_var=equal_var, nan_policy='omit')\n",
    "    decisao = \"rejeita H0 (médias diferentes)\" if p < alpha else \"falha em rejeitar H0 (sem diferença)\"\n",
    "    return {\n",
    "        \"t_statistic\": t_stat,\n",
    "        \"p_value\": p,\n",
    "        \"decisão\": decisao\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
